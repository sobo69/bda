{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyN/oe3DF9l8Bob6zrIHIqau"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"aFl0q7LkEPrq"},"outputs":[],"source":["\n","\n","from pyspark.sql import SparkSession\n","from pyspark.ml.feature import VectorAssembler, StandardScaler\n","from pyspark.ml.clustering import KMeans\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","import numpy as np\n","\n","# Step 1: Initialize Spark Session\n","spark = SparkSession.builder.appName(\"PimaKMeansClustering\").getOrCreate()\n","\n","# Step 2: Load dataset\n","file_path = \"pima.csv\"  # Adjust if needed\n","df = spark.read.csv(file_path, header=True, inferSchema=True)\n","\n","# Step 3: Choose features (plas: glucose, pres: blood pressure)\n","features = [\"plas\", \"pres\"]\n","\n","# Step 4: Assemble features\n","assembler = VectorAssembler(inputCols=features, outputCol=\"features\")\n","assembled_data = assembler.transform(df)\n","\n","# Step 5: Standardize features\n","scaler = StandardScaler(inputCol=\"features\", outputCol=\"scaledFeatures\", withStd=True, withMean=False)\n","scaler_model = scaler.fit(assembled_data)\n","scaled_data = scaler_model.transform(assembled_data)\n","\n","# Step 6: Elbow Method\n","wssse_list = []\n","k_values = list(range(2, 11))\n","\n","for k in k_values:\n","    kmeans = KMeans(featuresCol=\"scaledFeatures\", k=k)\n","    model = kmeans.fit(scaled_data)\n","    wssse = model.summary.trainingCost\n","    wssse_list.append(wssse)\n","    print(f\"k={k}, WSSSE={wssse}\")\n","\n","\n","# Step 8: Choose optimal k (based on elbow) â€” you can update this value manually\n","optimal_k = 3  # Change if needed based on elbow plot\n","\n","# Step 9: Fit KMeans with optimal k\n","kmeans = KMeans(featuresCol=\"scaledFeatures\", k=optimal_k)\n","model = kmeans.fit(scaled_data)\n","clusters = model.transform(scaled_data)\n","\n","# Step 10: Show predictions\n","clusters.select(\"plas\", \"pres\", \"prediction\").show(10)\n","\n","# Step 11: Convert to Pandas for plotting\n","clusters_pd = clusters.select(\"plas\", \"pres\", \"prediction\").toPandas()\n","\n","# Step 12: Get and inverse-transform centroids\n","std_values = scaler_model.std.toArray()\n","scaled_centers = np.array(model.clusterCenters())\n","original_centers = scaled_centers * std_values  # Inverse scaling\n","\n","# Step 13: Scatter plot with centroids\n","plt.figure(figsize=(8, 6))\n","sns.scatterplot(\n","    x=\"plas\", y=\"pres\", hue=\"prediction\", palette=\"viridis\", data=clusters_pd\n",")\n","plt.scatter(\n","    original_centers[:, 0], original_centers[:, 1],\n","    color='red', marker='X', s=200, label=\"Centroids\"\n",")\n","plt.title(f\"K-Means Clustering on Pima Dataset (k={optimal_k})\")\n","plt.xlabel(\"Plasma Glucose (plas)\")\n","plt.ylabel(\"Blood Pressure (pres)\")\n","plt.legend()\n","plt.show()\n","\n","# Step 14: Stop Spark session\n","spark.stop()\n",""]}]}